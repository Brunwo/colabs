{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Brunwo/colabs/blob/main/Serving_Foundation_Models_from_Hugging_Face_with_Ray_Serve%2C_MLflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I17bSxxg1evl"
      },
      "source": [
        "# Serving Foundation Models from Hugging Face with Ray Serve, MLflow\n",
        "\n",
        "Authored by: Jonathan Jin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuS0daXP1lIa"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "This notebook explores solutions for streamlining the deployment of models from a model registry. For teams that want to productionize many models over time, investments at this \"transition point\" in the AI/ML project lifecycle can meaningful drive down time-to-production. This can be important for a younger, smaller team that may not have the benefit of large swathes of existing infrastructure in place to form a \"golden path\" for serving online models in production.\n",
        "\n",
        "# Motivation\n",
        "\n",
        "Optimizing this stage of the model lifecycle is particularly important due to the production-facing aspect of the end result. At this stage, your model becomes, in effect, a microservice. This means that you now need to contend with all elements of service ownership, which can include:\n",
        "\n",
        "- Standardizing and enforcing API backwards-compatibility;\n",
        "- Logging, metrics, and general observability concerns;\n",
        "- Etc.\n",
        "\n",
        "Needing to repeat the same general-purpose setup each time you want to deploy a new model will result in development costs adding up significantly over time for you and your team. On the flip side, given the \"long tail\" of production-model ownership (assuming a productionized model is not likely to be decommissioned anytime soon), streamlining investments here can pay healthy dividends over time.\n",
        "\n",
        "Given all of the above, we motivate our exploration here with the following user story:\n",
        "\n",
        "> I would like to deploy a model from a model registry (such as [MLflow](https://mlflow.org/)) using **only the name of the model**. The less boilerplate and scaffolding that I need to replicate each time I want to deploy a new model,the better. I would like the ability to dynamically select between different versions of the model without needing to set up a whole new deployment to accommodate those new versions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXlB7AJr2foY"
      },
      "source": [
        "# Components\n",
        "\n",
        "For our exploration here, we'll use the following minimal stack:\n",
        "\n",
        "- MLflow for model registry;\n",
        "- Ray Serve for model serving.\n",
        "\n",
        "For demonstrative purposes, we'll exclusively use off-the-shelf open-source models from Hugging Face Hub.\n",
        "\n",
        "We will **not** use GPUs for inference because inference performance is orthogonal to our focus here today. Needless to say, in \"real life,\" you will likely not be able to get away with serving your model with CPU compute.\n",
        "\n",
        "Let's install our dependencies now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "HfLQGO6E2hnW",
        "outputId": "4ef0ca09-f62b-4cd0-8223-f8d623681411"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.2)\n",
            "Collecting mlflow-skinny\n",
            "  Downloading mlflow_skinny-2.22.0-py3-none-any.whl.metadata (31 kB)\n",
            "Collecting ray[serve]\n",
            "  Downloading ray-2.46.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: cachetools<6,>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny) (8.2.1)\n",
            "Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny) (3.1.1)\n",
            "Collecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny)\n",
            "  Downloading databricks_sdk-0.55.0-py3-none-any.whl.metadata (39 kB)\n",
            "Collecting fastapi<1 (from mlflow-skinny)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Requirement already satisfied: gitpython<4,>=3.1.9 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny) (3.1.44)\n",
            "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny) (8.7.0)\n",
            "Collecting opentelemetry-api<3,>=1.9.0 (from mlflow-skinny)\n",
            "  Downloading opentelemetry_api-1.33.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting opentelemetry-sdk<3,>=1.9.0 (from mlflow-skinny)\n",
            "  Downloading opentelemetry_sdk-1.33.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: protobuf<7,>=3.12.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny) (5.29.4)\n",
            "Requirement already satisfied: pydantic<3,>=1.10.8 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny) (2.11.4)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny) (0.5.3)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny) (4.13.2)\n",
            "Collecting uvicorn<1 (from mlflow-skinny)\n",
            "  Downloading uvicorn-0.34.3-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from ray[serve]) (4.23.0)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ray[serve]) (1.1.0)\n",
            "Requirement already satisfied: aiohttp>=3.7 in /usr/local/lib/python3.11/dist-packages (from ray[serve]) (3.11.15)\n",
            "Collecting starlette (from ray[serve])\n",
            "  Downloading starlette-0.47.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting py-spy>=0.2.0 (from ray[serve])\n",
            "  Downloading py_spy-0.4.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (16 kB)\n",
            "Collecting virtualenv!=20.21.1,>=20.0.24 (from ray[serve])\n",
            "  Downloading virtualenv-20.31.2-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting colorful (from ray[serve])\n",
            "  Downloading colorful-0.5.6-py2.py3-none-any.whl.metadata (16 kB)\n",
            "Collecting opencensus (from ray[serve])\n",
            "  Downloading opencensus-0.11.4-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Collecting aiohttp_cors (from ray[serve])\n",
            "  Downloading aiohttp_cors-0.8.1-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting watchfiles (from ray[serve])\n",
            "  Downloading watchfiles-1.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: smart_open in /usr/local/lib/python3.11/dist-packages (from ray[serve]) (7.1.0)\n",
            "Requirement already satisfied: prometheus_client>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from ray[serve]) (0.22.0)\n",
            "Requirement already satisfied: grpcio>=1.42.0 in /usr/local/lib/python3.11/dist-packages (from ray[serve]) (1.71.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.7->ray[serve]) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.7->ray[serve]) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.7->ray[serve]) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.7->ray[serve]) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.7->ray[serve]) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.7->ray[serve]) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.7->ray[serve]) (1.20.0)\n",
            "Requirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.11/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny) (2.38.0)\n",
            "Collecting starlette (from ray[serve])\n",
            "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython<4,>=3.1.9->mlflow-skinny) (4.0.12)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny) (3.21.0)\n",
            "Collecting deprecated>=1.2.6 (from opentelemetry-api<3,>=1.9.0->mlflow-skinny)\n",
            "  Downloading Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting importlib_metadata!=4.7.0,<9,>=3.7.0 (from mlflow-skinny)\n",
            "  Downloading importlib_metadata-8.6.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.54b1 (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny)\n",
            "  Downloading opentelemetry_semantic_conventions-0.54b1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette->ray[serve]) (4.9.0)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn<1->mlflow-skinny) (0.16.0)\n",
            "Collecting distlib<1,>=0.3.7 (from virtualenv!=20.21.1,>=20.0.24->ray[serve])\n",
            "  Downloading distlib-0.3.9-py2.py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.11/dist-packages (from virtualenv!=20.21.1,>=20.0.24->ray[serve]) (4.3.8)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray[serve]) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray[serve]) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray[serve]) (0.25.1)\n",
            "Collecting opencensus-context>=0.1.3 (from opencensus->ray[serve])\n",
            "  Downloading opencensus_context-0.1.3-py2.py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: six~=1.16 in /usr/local/lib/python3.11/dist-packages (from opencensus->ray[serve]) (1.17.0)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opencensus->ray[serve]) (2.24.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart_open->ray[serve]) (1.17.2)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]; extra == \"serve\"->ray[serve])\n",
            "  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting python-dotenv>=0.13 (from uvicorn[standard]; extra == \"serve\"->ray[serve])\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting uvloop>=0.15.1 (from uvicorn[standard]; extra == \"serve\"->ray[serve])\n",
            "  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]; extra == \"serve\"->ray[serve]) (15.0.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette->ray[serve]) (1.3.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny) (5.0.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[serve]) (1.70.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[serve]) (1.26.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny) (4.9.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny) (0.6.1)\n",
            "Downloading mlflow_skinny-2.22.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading databricks_sdk-0.55.0-py3-none-any.whl (722 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m722.9/722.9 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.33.1-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading importlib_metadata-8.6.1-py3-none-any.whl (26 kB)\n",
            "Downloading opentelemetry_sdk-1.33.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.0/119.0 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.54b1-py3-none-any.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.9/194.9 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading py_spy-0.4.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (2.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.34.3-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading virtualenv-20.31.2-py3-none-any.whl (6.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohttp_cors-0.8.1-py3-none-any.whl (25 kB)\n",
            "Downloading colorful-0.5.6-py2.py3-none-any.whl (201 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.4/201.4 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencensus-0.11.4-py2.py3-none-any.whl (128 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ray-2.46.0-cp311-cp311-manylinux2014_x86_64.whl (68.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.5/68.5 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (454 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
            "Downloading distlib-0.3.9-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencensus_context-0.1.3-py2.py3-none-any.whl (5.1 kB)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: py-spy, opencensus-context, distlib, colorful, virtualenv, uvloop, uvicorn, python-dotenv, importlib_metadata, httptools, deprecated, watchfiles, starlette, opentelemetry-api, opentelemetry-semantic-conventions, fastapi, databricks-sdk, aiohttp_cors, ray, opentelemetry-sdk, opencensus, mlflow-skinny\n",
            "  Attempting uninstall: importlib_metadata\n",
            "    Found existing installation: importlib_metadata 8.7.0\n",
            "    Uninstalling importlib_metadata-8.7.0:\n",
            "      Successfully uninstalled importlib_metadata-8.7.0\n",
            "Successfully installed aiohttp_cors-0.8.1 colorful-0.5.6 databricks-sdk-0.55.0 deprecated-1.2.18 distlib-0.3.9 fastapi-0.115.12 httptools-0.6.4 importlib_metadata-8.6.1 mlflow-skinny-2.22.0 opencensus-0.11.4 opencensus-context-0.1.3 opentelemetry-api-1.33.1 opentelemetry-sdk-1.33.1 opentelemetry-semantic-conventions-0.54b1 py-spy-0.4.0 python-dotenv-1.1.0 ray-2.46.0 starlette-0.46.2 uvicorn-0.34.3 uvloop-0.21.0 virtualenv-20.31.2 watchfiles-1.0.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "importlib_metadata"
                ]
              },
              "id": "3b6838432b134600b7a7c9b7eb84b33c"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install \"transformers\" \"mlflow-skinny\" \"ray[serve]\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0UziXBN4Szc"
      },
      "source": [
        "# Register the Model\n",
        "\n",
        "First, let's define the model that we'll use for our explorations today. For simplicity's sake, we'll use a simple text translation model, where the source and destination languages are configurable at registration time. In effect, this means that different \"versions\" of the model can be registered to translate different languages, but that the underlying model architecture and weights can stay the same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "D2HsBFUa4nBM",
        "outputId": "5202be3f-4aed-4b8d-adfd-39ba3160a6b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/mlflow/pyfunc/utils/data_validation.py:186: UserWarning: \u001b[33mAdd type hints to the `predict` method to enable data validation and automatic signature inference during model logging. Check https://mlflow.org/docs/latest/model/python_model.html#type-hint-usage-in-pythonmodel for more details.\u001b[0m\n",
            "  color_warning(\n"
          ]
        }
      ],
      "source": [
        "import mlflow\n",
        "from transformers import pipeline\n",
        "\n",
        "class MyTranslationModel(mlflow.pyfunc.PythonModel):\n",
        "    def load_context(self, context):\n",
        "        self.lang_from = context.model_config.get(\"lang_from\", \"en\")\n",
        "        self.lang_to = context.model_config.get(\"lang_to\", \"de\")\n",
        "\n",
        "        self.input_label: str = context.model_config.get(\"input_label\", \"prompt\")\n",
        "\n",
        "        self.model_ref: str = context.model_config.get(\"hfhub_name\", \"google-t5/t5-base\")\n",
        "\n",
        "        self.pipeline = pipeline(\n",
        "            f\"translation_{self.lang_from}_to_{self.lang_to}\",\n",
        "            self.model_ref,\n",
        "        )\n",
        "\n",
        "    def predict(self, context, model_input, params=None):\n",
        "        prompt = model_input[self.input_label].tolist()\n",
        "\n",
        "        return self.pipeline(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PFbVlpdIBHA"
      },
      "source": [
        "(You might be wondering why we even bothered making the input label configurable. This will be useful to us later.)\n",
        "\n",
        "Now that our model is defined, let's register an actual version of it. This particular version will use Google's [T5 Base](https://huggingface.co/google-t5/t5-base) model and be configured to translate from **English** to **German**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319,
          "referenced_widgets": [
            "32dc3acc814944d4838d029f029c0dd4",
            "b7dd8cc512a14d44a2dd4ad391326f49",
            "6331ddfcd8884f489046579014469b9b",
            "25e660422d374df7bfec9ffd20a20b50",
            "ac8ce7809cd44cf7b0f241930e4f6ac8",
            "381849c7134e435d840a813aed7d0171",
            "d343cfa03753414d8cc58732cc96abaa",
            "afcee86421514eba93585650c886385b",
            "a206f552c0354964b5bee0dd36a9a075",
            "65dbde4fd56f43ff9434a3e5e425c01b",
            "6b7b9e25a612468b9f4bdf3021b5fdf1",
            "c6aa0fd4f8604fd1a8091b07f6b4ebcb",
            "a876636a5b4f415b9f3eb261433947f8",
            "bca38994e0064985ab91d38548af0472",
            "59d1e0dca5df409b84655b730b44f5b7",
            "76d9fbf5589c456fa215f0432cd4586e",
            "ab83433647b04558a0d05ba2b9d89319",
            "36008e0b1d0741688e8faa3590284c99",
            "17b21fe12b9c44898bfdbbf027e4729a",
            "97bef391062a493c82f7b0ac45b06870",
            "57602187346c4a7ab9ee8b2d70b657db",
            "6fb7db576be647c98d1ac2d5d7f6590f",
            "1a159611f43b4ae1887499e7706e5e21",
            "c953b9fa725540a58d4b3adca3cde382",
            "55d6d9c832454cfbaf451047253acb8b",
            "0dd58e9e118248a783d97dfe4660dafc",
            "7e1e55b87627434085df121c255f4571",
            "07327813872f4da49192b311bdf6ca16",
            "97451267b0644213a14e1c132f8e32a0",
            "abac47606fc4476e8f809df304666969",
            "ded0dbe1e8634eefa56f6a7c2f727644",
            "826946eb7d7d4c4198b5b3dc4f486f5c",
            "93dc42a292de44078b8303a5e3f77a5d",
            "c5b79f27c8834eeeaa058f2d828f7609",
            "d8e521ec0d4443cab866d8ab58a72acb",
            "f28587577722483b93df1ad78265daa7",
            "244c671669654875ac39015db4e40a2b",
            "de20e832dd7447998e247ec2b11d9137",
            "dd7ce0f777c846cd9cc01d136524e790",
            "6dc1ae8f8c894145b68a1b964add2593",
            "a7c65a8bed184d6f99257fc8dd05fa1a",
            "77cd41f4b1f948bf9a580f89812ee888",
            "df247f98d36644588b3e0f2d3a4f7216",
            "7ee3568f2a5c43d3adfc8b1e7f50425f",
            "89d8044972fd45dda88ecfdd486b253c",
            "89aa211f82344388917c723c972caed2",
            "19a572661cb548f898bac60c3db071a2",
            "37453595a40f4341bbdd5f73fc99c09f",
            "f4ab17cc1e87461a96e3fd3ade429212",
            "274629bce09e46d28e82fdc156ce4c17",
            "24551216ab5a405ca47cc1d34224916b",
            "ffd4333705d44b14b4f58c370fc3faa1",
            "94468a5db6414622a285e1d54a41d712",
            "0061c8b5ea944f6ea504984f1b829f79",
            "6891f9df939f4c2c8ac6b7829750ee2b"
          ]
        },
        "id": "SpGCrnAx6eVf",
        "outputId": "5211f1c6-700d-4f5d-e5b1-2a3c2a6459dc"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "32dc3acc814944d4838d029f029c0dd4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c6aa0fd4f8604fd1a8091b07f6b4ebcb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1a159611f43b4ae1887499e7706e5e21"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c5b79f27c8834eeeaa058f2d828f7609"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "89d8044972fd45dda88ecfdd486b253c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "2025/06/03 10:08:57 INFO mlflow.pyfunc: Inferring model signature from input example\n",
            "Device set to use cuda:0\n",
            "Successfully registered model 'translation_model'.\n",
            "Created version '1' of model 'translation_model'.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "with mlflow.start_run():\n",
        "    model_info = mlflow.pyfunc.log_model(\n",
        "        \"translation_model\",\n",
        "        registered_model_name=\"translation_model\",\n",
        "        python_model=MyTranslationModel(),\n",
        "        pip_requirements=[\"transformers\"],\n",
        "        input_example=pd.DataFrame({\n",
        "            \"prompt\": [\"Hello my name is Jonathan.\"],\n",
        "        }),\n",
        "        model_config={\n",
        "            \"hfhub_name\": \"google-t5/t5-base\",\n",
        "            \"lang_from\": \"en\",\n",
        "            \"lang_to\": \"de\",\n",
        "        },\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaUwo6E0DPbI"
      },
      "source": [
        "Let's keep track of this exact version. This will be useful later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "e0o4ICh38Pjy"
      },
      "outputs": [],
      "source": [
        "en_to_de_version: str = str(model_info.registered_model_version)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jn0RU7fXDTdD"
      },
      "source": [
        "The registered model metadata contains some useful information for us. Most notably, the registered model version is associated with a strict **signature** that denotes the expected shape of its input and output. This will be useful to us later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKMgYR_jDhOA",
        "outputId": "86238db0-99b3-426a-811e-94c91c811743"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "inputs: \n",
              "  ['prompt': string (required)]\n",
              "outputs: \n",
              "  ['translation_text': string (required)]\n",
              "params: \n",
              "  None"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "model_info.signature"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwa3o-0B9FPO"
      },
      "source": [
        "# Serve the Model\n",
        "\n",
        "Now that our model is registered in MLflow, let's set up our serving scaffolding using [Ray Serve](https://docs.ray.io/en/latest/serve/index.html). For now, we'll limit our \"deployment\" to the following behavior:\n",
        "\n",
        "- Source the seleted model and version from MLflow;\n",
        "- Receive inference requests and return inference responses via a simple REST API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7OZ2lqOS9oqw"
      },
      "outputs": [],
      "source": [
        "import mlflow\n",
        "import pandas as pd\n",
        "\n",
        "from ray import serve\n",
        "from fastapi import FastAPI\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "@serve.deployment\n",
        "@serve.ingress(app)\n",
        "class ModelDeployment:\n",
        "    def __init__(self, model_name: str = \"translation_model\", default_version: str = \"1\"):\n",
        "        self.model_name = model_name\n",
        "        self.default_version = default_version\n",
        "\n",
        "        self.model = mlflow.pyfunc.load_model(f\"models:/{self.model_name}/{self.default_version}\")\n",
        "\n",
        "\n",
        "    @app.post(\"/serve\")\n",
        "    async def serve(self, input_string: str):\n",
        "        return self.model.predict(pd.DataFrame({\"prompt\": [input_string]}))\n",
        "\n",
        "deployment = ModelDeployment.bind(default_version=en_to_de_version)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f018wd2fEia7"
      },
      "source": [
        "You might have notice that hard-coding `\"prompt\"` as the input label here introduces hidden coupling between the registered model's signature and the deployment implementation. We'll come back to this later.\n",
        "\n",
        "Now, let's run the deployment and play around with it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MudMnivd_DrC",
        "outputId": "e6a86ae8-702d-4071-cccd-6dd402b77cee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-06-03 10:09:31,568\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
            "\u001b[36m(ProxyActor pid=2988)\u001b[0m INFO 2025-06-03 10:09:36,205 proxy 172.28.0.12 -- Proxy starting on node c054e0b5b151b66da7e3c3ee08b70e0f1728e400f702fa485864fc71 (HTTP port: 8000).\n",
            "\u001b[36m(ProxyActor pid=2988)\u001b[0m INFO 2025-06-03 10:09:36,248 proxy 172.28.0.12 -- Got updated endpoints: {}.\n",
            "INFO 2025-06-03 10:09:36,343 serve 2104 -- Started Serve in namespace \"serve\".\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m INFO 2025-06-03 10:09:36,445 controller 2989 -- Deploying new version of Deployment(name='ModelDeployment', app='default') (initial target replicas: 1).\n",
            "\u001b[36m(ProxyActor pid=2988)\u001b[0m INFO 2025-06-03 10:09:36,449 proxy 172.28.0.12 -- Got updated endpoints: {Deployment(name='ModelDeployment', app='default'): EndpointInfo(route='/', app_is_cross_language=False)}.\n",
            "\u001b[36m(ProxyActor pid=2988)\u001b[0m INFO 2025-06-03 10:09:36,464 proxy 172.28.0.12 -- Started <ray.serve._private.router.SharedRouterLongPollClient object at 0x799225325d90>.\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m INFO 2025-06-03 10:09:36,550 controller 2989 -- Adding 1 replica to Deployment(name='ModelDeployment', app='default').\n",
            "\u001b[36m(ServeReplica:default:ModelDeployment pid=3167)\u001b[0m 2025-06-03 10:09:47.742689: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\u001b[36m(ServeReplica:default:ModelDeployment pid=3167)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "\u001b[36m(ServeReplica:default:ModelDeployment pid=3167)\u001b[0m E0000 00:00:1748945387.767067    3217 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\u001b[36m(ServeReplica:default:ModelDeployment pid=3167)\u001b[0m E0000 00:00:1748945387.774760    3217 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m ERROR 2025-06-03 10:09:52,896 controller 2989 -- Exception in Replica(id='3w4o2nli', deployment='ModelDeployment', app='default'), the replica will be stopped.\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m Traceback (most recent call last):\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/ray/serve/_private/deployment_state.py\", line 694, in check_ready\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m     ) = ray.get(self._ready_obj_ref)\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m     return fn(*args, **kwargs)\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m            ^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m     return func(*args, **kwargs)\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/ray/_private/worker.py\", line 2822, in get\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/ray/_private/worker.py\", line 930, in get_objects\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m     raise value.as_instanceof_cause()\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ServeReplica:default:ModelDeployment.initialize_and_get_metadata()\u001b[39m (pid=3167, ip=172.28.0.12, actor_id=97be337e9bd8a72bde5cbdfe01000000, repr=<ray.serve._private.replica.ServeReplica:default:ModelDeployment object at 0x7a0de59c9c90>)\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m   File \"/usr/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m     return self.__get_result()\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m            ^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m   File \"/usr/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m     raise self._exception\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/ray/serve/_private/replica.py\", line 984, in initialize_and_get_metadata\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m     await self._replica_impl.initialize(deployment_config)\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/ray/serve/_private/replica.py\", line 713, in initialize\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m     raise RuntimeError(traceback.format_exc()) from None\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m RuntimeError: Traceback (most recent call last):\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/ray/serve/_private/replica.py\", line 690, in initialize\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m     self._user_callable_asgi_app = await asyncio.wrap_future(\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/ray/serve/_private/replica.py\", line 1384, in initialize_callable\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m     await self._call_func_or_gen(\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/ray/serve/_private/replica.py\", line 1345, in _call_func_or_gen\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m     result = callable(*args, **kwargs)\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/ray/serve/api.py\", line 223, in __init__\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m     cls.__init__(self, *args, **kwargs)\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m   File \"<ipython-input-5-985b8b8fd24c>\", line 16, in __init__\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/mlflow/tracing/provider.py\", line 422, in wrapper\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m     is_func_called, result = True, f(*args, **kwargs)\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m                                    ^^^^^^^^^^^^^^^^^^\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/mlflow/pyfunc/__init__.py\", line 1131, in load_model\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m     model_impl = importlib.import_module(conf[MAIN])._load_pyfunc(data_path, model_config)\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/mlflow/pyfunc/model.py\", line 1090, in _load_pyfunc\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m     context, python_model, signature = _load_context_model_and_signature(model_path, model_config)\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/mlflow/pyfunc/model.py\", line 1073, in _load_context_model_and_signature\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m     python_model = cloudpickle.load(f)\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m                    ^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/storage.py\", line 533, in _load_from_bytes\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m     return torch.load(io.BytesIO(b), weights_only=False)\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/serialization.py\", line 1495, in load\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m     return _legacy_load(\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m            ^^^^^^^^^^^^^\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/serialization.py\", line 1754, in _legacy_load\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m     result = unpickler.load()\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m              ^^^^^^^^^^^^^^^^\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/serialization.py\", line 1682, in persistent_load\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m     obj = restore_location(obj, location)\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/serialization.py\", line 693, in default_restore_location\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m     result = fn(storage, location)\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m              ^^^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/serialization.py\", line 631, in _deserialize\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m     device = _validate_device(location, backend_name)\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/serialization.py\", line 600, in _validate_device\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m     raise RuntimeError(\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m RuntimeError: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m INFO 2025-06-03 10:09:52,908 controller 2989 -- Adding 1 replica to Deployment(name='ModelDeployment', app='default').\n",
            "\u001b[36m(ServeReplica:default:ModelDeployment pid=3167)\u001b[0m ERROR 2025-06-03 10:09:52,908 default_ModelDeployment 3w4o2nli -- Exception during graceful shutdown of replica: 'ModelDeployment' object has no attribute '_serve_asgi_lifespan'\n",
            "\u001b[36m(ServeReplica:default:ModelDeployment pid=3167)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/ray/serve/_private/replica.py\", line 1721, in call_destructor\n",
            "\u001b[36m(ServeReplica:default:ModelDeployment pid=3167)\u001b[0m     result = await result\n",
            "\u001b[36m(ServeReplica:default:ModelDeployment pid=3167)\u001b[0m              ^^^^^^^^^^^^\n",
            "\u001b[36m(ServeReplica:default:ModelDeployment pid=3167)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/ray/serve/api.py\", line 229, in __del__\n",
            "\u001b[36m(ServeReplica:default:ModelDeployment pid=3167)\u001b[0m     await ASGIAppReplicaWrapper.__del__(self)\n",
            "\u001b[36m(ServeReplica:default:ModelDeployment pid=3167)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/ray/serve/_private/http_util.py\", line 515, in __del__\n",
            "\u001b[36m(ServeReplica:default:ModelDeployment pid=3167)\u001b[0m     with LoggingContext(self._serve_asgi_lifespan.logger, level=logging.WARNING):\n",
            "\u001b[36m(ServeReplica:default:ModelDeployment pid=3167)\u001b[0m AttributeError: 'ModelDeployment' object has no attribute '_serve_asgi_lifespan'\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m INFO 2025-06-03 10:09:53,025 controller 2989 -- Replica(id='3w4o2nli', deployment='ModelDeployment', app='default') is stopped.\n",
            "\u001b[36m(ServeReplica:default:ModelDeployment pid=3283)\u001b[0m 2025-06-03 10:10:03.510454: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\u001b[36m(ServeReplica:default:ModelDeployment pid=3283)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "\u001b[36m(ServeReplica:default:ModelDeployment pid=3283)\u001b[0m E0000 00:00:1748945403.535746    3331 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\u001b[36m(ServeReplica:default:ModelDeployment pid=3283)\u001b[0m E0000 00:00:1748945403.547710    3331 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\u001b[36m(ServeReplica:default:ModelDeployment pid=3167)\u001b[0m Traceback (most recent call last):\n",
            "\u001b[36m(ServeReplica:default:ModelDeployment pid=3167)\u001b[0m     await self._call_func_or_gen(\n",
            "\u001b[36m(ServeReplica:default:ModelDeployment pid=3167)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/ray/serve/_private/replica.py\", line 1347, in _call_func_or_gen\n",
            "\u001b[36m(ServeReplica:default:ModelDeployment pid=3167)\u001b[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m ERROR 2025-06-03 10:10:07,862 controller 2989 -- Exception in Replica(id='0ffv68vx', deployment='ModelDeployment', app='default'), the replica will be stopped.\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m Traceback (most recent call last):\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/ray/serve/_private/deployment_state.py\", line 694, in check_ready\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m     ) = ray.get(self._ready_obj_ref)\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m     return fn(*args, **kwargs)\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m            ^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m     return func(*args, **kwargs)\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/ray/_private/worker.py\", line 2822, in get\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/ray/_private/worker.py\", line 930, in get_objects\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m     raise value.as_instanceof_cause()\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ServeReplica:default:ModelDeployment.initialize_and_get_metadata()\u001b[39m (pid=3283, ip=172.28.0.12, actor_id=02b21cadab16688ca638cc3701000000, repr=<ray.serve._private.replica.ServeReplica:default:ModelDeployment object at 0x7b2675213610>)\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m   File \"/usr/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m     return self.__get_result()\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m            ^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m   File \"/usr/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m     raise self._exception\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/ray/serve/_private/replica.py\", line 984, in initialize_and_get_metadata\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m     await self._replica_impl.initialize(deployment_config)\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/ray/serve/_private/replica.py\", line 713, in initialize\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m     raise RuntimeError(traceback.format_exc()) from None\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m RuntimeError: Traceback (most recent call last):\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/ray/serve/_private/replica.py\", line 690, in initialize\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m     self._user_callable_asgi_app = await asyncio.wrap_future(\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/ray/serve/_private/replica.py\", line 1384, in initialize_callable\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m     await self._call_func_or_gen(\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/ray/serve/_private/replica.py\", line 1345, in _call_func_or_gen\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m     result = callable(*args, **kwargs)\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/ray/serve/api.py\", line 223, in __init__\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m     cls.__init__(self, *args, **kwargs)\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m   File \"<ipython-input-5-985b8b8fd24c>\", line 16, in __init__\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/mlflow/tracing/provider.py\", line 422, in wrapper\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m     is_func_called, result = True, f(*args, **kwargs)\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m                                    ^^^^^^^^^^^^^^^^^^\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/mlflow/pyfunc/__init__.py\", line 1131, in load_model\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m     model_impl = importlib.import_module(conf[MAIN])._load_pyfunc(data_path, model_config)\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/mlflow/pyfunc/model.py\", line 1090, in _load_pyfunc\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m     context, python_model, signature = _load_context_model_and_signature(model_path, model_config)\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/mlflow/pyfunc/model.py\", line 1073, in _load_context_model_and_signature\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m     python_model = cloudpickle.load(f)\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m                    ^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/storage.py\", line 533, in _load_from_bytes\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m     return torch.load(io.BytesIO(b), weights_only=False)\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/serialization.py\", line 1495, in load\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m     return _legacy_load(\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m            ^^^^^^^^^^^^^\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/serialization.py\", line 1754, in _legacy_load\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m     result = unpickler.load()\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m              ^^^^^^^^^^^^^^^^\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/serialization.py\", line 1682, in persistent_load\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m     obj = restore_location(obj, location)\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/serialization.py\", line 693, in default_restore_location\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m     result = fn(storage, location)\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m              ^^^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/serialization.py\", line 631, in _deserialize\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m     device = _validate_device(location, backend_name)\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/serialization.py\", line 600, in _validate_device\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m     raise RuntimeError(\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m RuntimeError: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m INFO 2025-06-03 10:10:07,864 controller 2989 -- Adding 1 replica to Deployment(name='ModelDeployment', app='default').\n",
            "\u001b[36m(ServeReplica:default:ModelDeployment pid=3283)\u001b[0m ERROR 2025-06-03 10:10:07,865 default_ModelDeployment 0ffv68vx -- Exception during graceful shutdown of replica: 'ModelDeployment' object has no attribute '_serve_asgi_lifespan'\n",
            "\u001b[36m(ServeReplica:default:ModelDeployment pid=3283)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/ray/serve/_private/replica.py\", line 1721, in call_destructor\n",
            "\u001b[36m(ServeReplica:default:ModelDeployment pid=3283)\u001b[0m     result = await result\n",
            "\u001b[36m(ServeReplica:default:ModelDeployment pid=3283)\u001b[0m              ^^^^^^^^^^^^\n",
            "\u001b[36m(ServeReplica:default:ModelDeployment pid=3283)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/ray/serve/api.py\", line 229, in __del__\n",
            "\u001b[36m(ServeReplica:default:ModelDeployment pid=3283)\u001b[0m     await ASGIAppReplicaWrapper.__del__(self)\n",
            "\u001b[36m(ServeReplica:default:ModelDeployment pid=3283)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/ray/serve/_private/http_util.py\", line 515, in __del__\n",
            "\u001b[36m(ServeReplica:default:ModelDeployment pid=3283)\u001b[0m     with LoggingContext(self._serve_asgi_lifespan.logger, level=logging.WARNING):\n",
            "\u001b[36m(ServeReplica:default:ModelDeployment pid=3283)\u001b[0m AttributeError: 'ModelDeployment' object has no attribute '_serve_asgi_lifespan'\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m INFO 2025-06-03 10:10:07,976 controller 2989 -- Replica(id='0ffv68vx', deployment='ModelDeployment', app='default') is stopped.\n",
            "\u001b[36m(ServeReplica:default:ModelDeployment pid=3392)\u001b[0m 2025-06-03 10:10:18.127880: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\u001b[36m(ServeReplica:default:ModelDeployment pid=3392)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "\u001b[36m(ServeReplica:default:ModelDeployment pid=3392)\u001b[0m E0000 00:00:1748945418.151760    3438 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\u001b[36m(ServeReplica:default:ModelDeployment pid=3283)\u001b[0m Traceback (most recent call last):\n",
            "\u001b[36m(ServeReplica:default:ModelDeployment pid=3283)\u001b[0m     await self._call_func_or_gen(\n",
            "\u001b[36m(ServeReplica:default:ModelDeployment pid=3283)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/ray/serve/_private/replica.py\", line 1347, in _call_func_or_gen\n",
            "\u001b[36m(ServeReplica:default:ModelDeployment pid=3283)\u001b[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[36m(ServeReplica:default:ModelDeployment pid=3392)\u001b[0m E0000 00:00:1748945418.161243    3438 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m ERROR 2025-06-03 10:10:32,130 controller 2989 -- Exception in Replica(id='cr4zc8k4', deployment='ModelDeployment', app='default'), the replica will be stopped.\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m Traceback (most recent call last):\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/ray/serve/_private/deployment_state.py\", line 694, in check_ready\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m     ) = ray.get(self._ready_obj_ref)\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m     return fn(*args, **kwargs)\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m            ^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m     return func(*args, **kwargs)\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/ray/_private/worker.py\", line 2822, in get\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/ray/_private/worker.py\", line 930, in get_objects\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m     raise value.as_instanceof_cause()\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ServeReplica:default:ModelDeployment.initialize_and_get_metadata()\u001b[39m (pid=3392, ip=172.28.0.12, actor_id=3a902c116c72170dc1fe41bd01000000, repr=<ray.serve._private.replica.ServeReplica:default:ModelDeployment object at 0x7cc94c6716d0>)\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m   File \"/usr/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m     return self.__get_result()\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m            ^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m   File \"/usr/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m     raise self._exception\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/ray/serve/_private/replica.py\", line 984, in initialize_and_get_metadata\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m     await self._replica_impl.initialize(deployment_config)\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/ray/serve/_private/replica.py\", line 713, in initialize\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m     raise RuntimeError(traceback.format_exc()) from None\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m RuntimeError: Traceback (most recent call last):\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/ray/serve/_private/replica.py\", line 690, in initialize\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m     self._user_callable_asgi_app = await asyncio.wrap_future(\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/ray/serve/_private/replica.py\", line 1384, in initialize_callable\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m     await self._call_func_or_gen(\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/ray/serve/_private/replica.py\", line 1345, in _call_func_or_gen\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m     result = callable(*args, **kwargs)\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/ray/serve/api.py\", line 223, in __init__\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m     cls.__init__(self, *args, **kwargs)\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m   File \"<ipython-input-5-985b8b8fd24c>\", line 16, in __init__\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/mlflow/tracing/provider.py\", line 422, in wrapper\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m     is_func_called, result = True, f(*args, **kwargs)\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m                                    ^^^^^^^^^^^^^^^^^^\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/mlflow/pyfunc/__init__.py\", line 1131, in load_model\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m     model_impl = importlib.import_module(conf[MAIN])._load_pyfunc(data_path, model_config)\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/mlflow/pyfunc/model.py\", line 1090, in _load_pyfunc\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m     context, python_model, signature = _load_context_model_and_signature(model_path, model_config)\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/mlflow/pyfunc/model.py\", line 1073, in _load_context_model_and_signature\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m     python_model = cloudpickle.load(f)\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m                    ^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/storage.py\", line 533, in _load_from_bytes\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m     return torch.load(io.BytesIO(b), weights_only=False)\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/serialization.py\", line 1495, in load\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m     return _legacy_load(\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m            ^^^^^^^^^^^^^\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/serialization.py\", line 1754, in _legacy_load\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m     result = unpickler.load()\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m              ^^^^^^^^^^^^^^^^\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/serialization.py\", line 1682, in persistent_load\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m     obj = restore_location(obj, location)\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/serialization.py\", line 693, in default_restore_location\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m     result = fn(storage, location)\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m              ^^^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/serialization.py\", line 631, in _deserialize\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m     device = _validate_device(location, backend_name)\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/serialization.py\", line 600, in _validate_device\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m     raise RuntimeError(\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m RuntimeError: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m ERROR 2025-06-03 10:10:32,136 controller 2989 -- Failed to update the deployments ['ModelDeployment'].\n",
            "\u001b[36m(ServeReplica:default:ModelDeployment pid=3392)\u001b[0m ERROR 2025-06-03 10:10:32,133 default_ModelDeployment cr4zc8k4 -- Exception during graceful shutdown of replica: 'ModelDeployment' object has no attribute '_serve_asgi_lifespan'\n",
            "\u001b[36m(ServeReplica:default:ModelDeployment pid=3392)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/ray/serve/_private/replica.py\", line 1721, in call_destructor\n",
            "\u001b[36m(ServeReplica:default:ModelDeployment pid=3392)\u001b[0m     result = await result\n",
            "\u001b[36m(ServeReplica:default:ModelDeployment pid=3392)\u001b[0m              ^^^^^^^^^^^^\n",
            "\u001b[36m(ServeReplica:default:ModelDeployment pid=3392)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/ray/serve/api.py\", line 229, in __del__\n",
            "\u001b[36m(ServeReplica:default:ModelDeployment pid=3392)\u001b[0m     await ASGIAppReplicaWrapper.__del__(self)\n",
            "\u001b[36m(ServeReplica:default:ModelDeployment pid=3392)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/ray/serve/_private/http_util.py\", line 515, in __del__\n",
            "\u001b[36m(ServeReplica:default:ModelDeployment pid=3392)\u001b[0m     with LoggingContext(self._serve_asgi_lifespan.logger, level=logging.WARNING):\n",
            "\u001b[36m(ServeReplica:default:ModelDeployment pid=3392)\u001b[0m AttributeError: 'ModelDeployment' object has no attribute '_serve_asgi_lifespan'\n",
            "\u001b[36m(ServeController pid=2989)\u001b[0m INFO 2025-06-03 10:10:32,239 controller 2989 -- Replica(id='cr4zc8k4', deployment='ModelDeployment', app='default') is stopped.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Deploying application default failed: Failed to update the deployments ['ModelDeployment'].",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-39a4b81d95a6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mserve\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeployment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ray/serve/api.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(target, blocking, name, route_prefix, logging_config, _local_testing_mode)\u001b[0m\n\u001b[1;32m    623\u001b[0m         \u001b[0mDeploymentHandle\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mcan\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m \u001b[0mto\u001b[0m \u001b[0mcall\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mapplication\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m     \"\"\"\n\u001b[0;32m--> 625\u001b[0;31m     handle = _run(\n\u001b[0m\u001b[1;32m    626\u001b[0m         \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ray/serve/api.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(target, _blocking, name, route_prefix, logging_config, _local_testing_mode)\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[0mcode\u001b[0m \u001b[0mindefinitely\u001b[0m \u001b[0muntil\u001b[0m \u001b[0mCtrl\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mC\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m     \"\"\"\n\u001b[0;32m--> 535\u001b[0;31m     return _run_many(\n\u001b[0m\u001b[1;32m    536\u001b[0m         [\n\u001b[1;32m    537\u001b[0m             RunTarget(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ray/serve/api.py\u001b[0m in \u001b[0;36m_run_many\u001b[0;34m(targets, wait_for_ingress_deployment_creation, wait_for_applications_running, _local_testing_mode)\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0mServeUsageTag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAPI_VERSION\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"v2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 513\u001b[0;31m         return client.deploy_applications(\n\u001b[0m\u001b[1;32m    514\u001b[0m             \u001b[0mbuilt_apps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0mwait_for_ingress_deployment_creation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwait_for_ingress_deployment_creation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ray/serve/_private/client.py\u001b[0m in \u001b[0;36mcheck\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRayServeException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Client has already been shut down.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcheck\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ray/serve/_private/client.py\u001b[0m in \u001b[0;36mdeploy_applications\u001b[0;34m(self, built_apps, wait_for_ingress_deployment_creation, wait_for_applications_running)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwait_for_applications_running\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_application_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroute_prefix\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m                     \u001b[0murl_part\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" at \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_root_url\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroute_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ray/serve/_private/client.py\u001b[0m in \u001b[0;36m_wait_for_application_running\u001b[0;34m(self, name, timeout_s)\u001b[0m\n\u001b[1;32m    233\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp_status\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mApplicationStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEPLOY_FAILED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m                 raise RuntimeError(\n\u001b[0m\u001b[1;32m    236\u001b[0m                     \u001b[0;34mf\"Deploying application {name} failed: {status.app_status.message}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m                 )\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Deploying application default failed: Failed to update the deployments ['ModelDeployment']."
          ]
        }
      ],
      "source": [
        "serve.run(deployment, blocking=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"CUDA devices: {torch.cuda.device_count()}\")"
      ],
      "metadata": {
        "id": "OGExTllJRsrl",
        "outputId": "4ece97af-6fb1-41c5-ed29-be488ec60d18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True\n",
            "CUDA devices: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTk1E5pp_gRz"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "requests.post(\n",
        "    \"http://127.0.0.1:8000/serve/\",\n",
        "    params={\"input_string\": \"The weather is lovely today\"},\n",
        ").json()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3CNI-mmE_22"
      },
      "source": [
        "This works fine, but you might have noticed that the REST API does not line up with the model signature. Namely, it uses the label `\"input_string\"` while the served model version itself uses the input label `\"prompt\"`. Similarly, the model can accept multiple inputs values, but the API only accepts one.\n",
        "\n",
        "If this feels [smelly](https://en.wikipedia.org/wiki/Code_smell) to you, keep reading; we'll come back to this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsJ65rNNDMVj"
      },
      "source": [
        "# Multiple Versions, One Endpoint\n",
        "\n",
        "Now we've got a basic endpoint set up for our model. Great! However, notice that this deployment is strictly tethered to a single version of this model -- specifically, version `1` of the registered `translation_model`.\n",
        "\n",
        "Imagine, now, that your team would like to come back and refine this model -- maybe retrain it on new data, or configure it to translate to a new language, e.g. French instead of German. Both would result in a new version of the `translation_model` getting registered. However, with our current deployment implementation, we'd need to set up a whole new endpoint for `translation_model/2`, require our users to remember which address and port corresponds to which version of the model, and so on. In other words: very cumbersome, very error-prone, very [toilsome](https://leaddev.com/velocity/what-toil-and-why-it-damaging-your-engineering-org).\n",
        "\n",
        "Conversely, imagine a scenario where we could reuse the exact same endpoint -- same signature, same address and port, same query conventions, etc. -- to serve both versions of this model. Our user can simply specify which version of the model they'd like to use, and we can treat one of them as the \"default\" in cases where the user didn't explicitly request one.\n",
        "\n",
        "This is one area where Ray Serve shines with a feature it calls [model multiplexing](https://docs.ray.io/en/latest/serve/model-multiplexing.html). In effect, this allows you to load up multiple \"versions\" of your model, dynamically hot-swapping them as needed, as well as unloading the versions that don't get used after some time. Very space-efficient, in other words.\n",
        "\n",
        "Let's try registering another version of the model -- this time, one that translates from English to French. We'll register this under the version `\"2\"`; the model server will retrieve the model version that way.\n",
        "\n",
        "But first, let's extend the model server with multiplexing support."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8GcI3WLE3Sc"
      },
      "outputs": [],
      "source": [
        "from ray import serve\n",
        "from fastapi import FastAPI\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "@serve.deployment\n",
        "@serve.ingress(app)\n",
        "class MultiplexedModelDeployment:\n",
        "\n",
        "    @serve.multiplexed(max_num_models_per_replica=2)\n",
        "    async def get_model(self, version: str):\n",
        "        return mlflow.pyfunc.load_model(f\"models:/{self.model_name}/{version}\")\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_name: str = \"translation_model\",\n",
        "        default_version: str = en_to_de_version,\n",
        "    ):\n",
        "        self.model_name = model_name\n",
        "        self.default_version = default_version\n",
        "\n",
        "    @app.post(\"/serve\")\n",
        "    async def serve(self, input_string: str):\n",
        "        model = await self.get_model(serve.get_multiplexed_model_id())\n",
        "        return model.predict(pd.DataFrame({\"prompt\": [input_string]}))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-gisRU_FKlJ"
      },
      "outputs": [],
      "source": [
        "multiplexed_deployment = MultiplexedModelDeployment.bind(model_name=\"translation_model\")\n",
        "serve.run(multiplexed_deployment, blocking=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qs7snXhxdlUR"
      },
      "source": [
        "Now let's actually register the new model version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3_essFBEuCo"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "with mlflow.start_run():\n",
        "    model_info = mlflow.pyfunc.log_model(\n",
        "        \"translation_model\",\n",
        "        registered_model_name=\"translation_model\",\n",
        "        python_model=MyTranslationModel(),\n",
        "        pip_requirements=[\"transformers\"],\n",
        "        input_example=pd.DataFrame({\n",
        "            \"prompt\": [\n",
        "                \"Hello my name is Jon.\",\n",
        "            ],\n",
        "        }),\n",
        "        model_config={\n",
        "            \"hfhub_name\": \"google-t5/t5-base\",\n",
        "            \"lang_from\": \"en\",\n",
        "            \"lang_to\": \"fr\",\n",
        "        },\n",
        "    )\n",
        "\n",
        "en_to_fr_version: str = str(model_info.registered_model_version)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxOzkg65dnZW"
      },
      "source": [
        "Now that that's registered, we can query for it via the model server like so..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EyeLmnPJFuRH",
        "outputId": "9dfb8df0-f207-42ae-b78b-db51d8843c15"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(ServeReplica:default:MultiplexedModelDeployment pid=32383)\u001b[0m INFO 2024-12-23 16:01:41,179 default_MultiplexedModelDeployment hnpendkt 1943df13-e56a-47d0-a49f-55fb78aa665b -- POST /serve/ 307 4.3ms\n",
            "\u001b[36m(ServeReplica:default:MultiplexedModelDeployment pid=32383)\u001b[0m INFO 2024-12-23 16:01:43,214 default_MultiplexedModelDeployment hnpendkt ee559e3e-a71d-48aa-8c24-10de5d7ad7df -- Loading model '15'.\n",
            "\u001b[36m(ServeReplica:default:MultiplexedModelDeployment pid=32383)\u001b[0m 2024-12-23 16:01:52.414753: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\u001b[36m(ServeReplica:default:MultiplexedModelDeployment pid=32383)\u001b[0m 2024-12-23 16:01:52.472202: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\u001b[36m(ServeReplica:default:MultiplexedModelDeployment pid=32383)\u001b[0m 2024-12-23 16:01:52.491131: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\u001b[36m(ServeReplica:default:MultiplexedModelDeployment pid=32383)\u001b[0m 2024-12-23 16:01:55.152832: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(ServeReplica:default:MultiplexedModelDeployment pid=32383)\u001b[0m Device set to use cpu\n",
            "\u001b[36m(ServeReplica:default:MultiplexedModelDeployment pid=32383)\u001b[0m INFO 2024-12-23 16:02:00,506 default_MultiplexedModelDeployment hnpendkt ee559e3e-a71d-48aa-8c24-10de5d7ad7df -- Successfully loaded model '15' in 17292.0ms.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'translation_text': \"Le temps est beau aujourd'hui\"}]"
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "requests.post(\n",
        "    \"http://127.0.0.1:8000/serve/\",\n",
        "    params={\"input_string\": \"The weather is lovely today\"},\n",
        "    headers={\"serve_multiplexed_model_id\": en_to_fr_version},\n",
        ").json()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVMCS4CedudN"
      },
      "source": [
        "Note how we were able to immediately access the model version **without redeploying the model server**. Ray Serve's multiplexing capabilities allow it to dynamically fetch the model weights in a just-in-time fashion; if I never requested version 2, it never gets loaded. This helps conserve compute resources for the models that **do** get queried. What's even more useful is that, if the number of models loaded up exceeds the configured maximum (`max_num_models_per_replica`), the [least-recently used model version will get evicted](https://docs.ray.io/en/latest/serve/model-multiplexing.html#why-model-multiplexing).\n",
        "\n",
        "Given that we set `max_num_models_per_replica=2` above, the \"default\" English-to-German version of the model should still be loaded up and readily available to serve requests without any cold-start time. Let's confirm that now:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEJFQNlwGGKh",
        "outputId": "b847d92e-fe0f-4439-bd87-e5773680c4d1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(ServeReplica:default:MultiplexedModelDeployment pid=32383)\u001b[0m INFO 2024-12-23 16:02:13,267 default_MultiplexedModelDeployment hnpendkt 8e680170-df74-49ba-856c-a7e9009abaab -- POST /serve/ 307 26.0ms\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'translation_text': 'Das Wetter ist heute nett.'}]"
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "requests.post(\n",
        "    \"http://127.0.0.1:8000/serve/\",\n",
        "    params={\"input_string\": \"The weather is lovely today\"},\n",
        "    headers={\"serve_multiplexed_model_id\": en_to_de_version},\n",
        ").json()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8CgPXcsIg5C"
      },
      "source": [
        "## Auto-Signature\n",
        "\n",
        "This is all well and good. However, notice that the following friction point still exists: when defining the server, we need to define a whole new signature for the API itself. At best, this is just some code duplication of the model signature itself (which is registered in MLflow). At worst, this can result in inconsistent APIs across all models that your team or organization owns, which can cause confusion and frustration in your downstream dependencies.\n",
        "\n",
        "In this particular case, it means that `MultiplexedModelDeployment` is secretly actually **tightly coupled** to the use-case for `translation_model`. What if we wanted to deploy another set of models that don't have to do with language translation? The defined `/serve` API, which returns a JSON object that looks like `{\"translated_text\": \"foo\"}`, would no longer make sense.\n",
        "\n",
        "To address this issue, **what if the API signature for `MultiplexedModelDeployment` could automatically mirror the signature of the underlying models it's serving**?\n",
        "\n",
        "Thankfully, with MLflow Model Registry metadata and some Python dynamic-class-creation shenanigans, this is entirely possible.\n",
        "\n",
        "Let's set things up so that the model server signature is inferred from the registered model itself. Since different versions of an MLflow can have different signatures, we'll use the \"default version\" to \"pin\" the signature; any attempt to multiplex an incompatible-signature model version we will have throw an error.\n",
        "\n",
        "Since Ray Serve binds the request and response signatures at class-definition time, we will use a Python metaclass to set this as a function of the specified model name and default model version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9GPbQrnP7OD"
      },
      "outputs": [],
      "source": [
        "import mlflow\n",
        "import pydantic\n",
        "\n",
        "def schema_to_pydantic(schema: mlflow.types.schema.Schema, *, name: str) -> pydantic.BaseModel:\n",
        "    return pydantic.create_model(\n",
        "        name,\n",
        "        **{\n",
        "            k: (v.type.to_python(), pydantic.Field(required=True))\n",
        "            for k, v in schema.input_dict().items()\n",
        "        }\n",
        "    )\n",
        "\n",
        "def get_req_resp_signatures(model_signature: mlflow.models.ModelSignature) -> tuple[pydantic.BaseModel, pydantic.BaseModel]:\n",
        "    inputs: mlflow.types.schema.Schema = model_signature.inputs\n",
        "    outputs: mlflow.types.schema.Schema = model_signature.outputs\n",
        "\n",
        "    return (schema_to_pydantic(inputs, name=\"InputModel\"), schema_to_pydantic(outputs, name=\"OutputModel\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PgetOY1LKp6m"
      },
      "outputs": [],
      "source": [
        "import mlflow\n",
        "\n",
        "from fastapi import FastAPI, Response, status\n",
        "from ray import serve\n",
        "from typing import List\n",
        "\n",
        "def deployment_from_model_name(model_name: str, default_version: str = \"1\"):\n",
        "    app = FastAPI()\n",
        "    model_info = mlflow.models.get_model_info(f\"models:/{model_name}/{default_version}\")\n",
        "    input_datamodel, output_datamodel = get_req_resp_signatures(model_info.signature)\n",
        "\n",
        "    @serve.deployment\n",
        "    @serve.ingress(app)\n",
        "    class DynamicallyDefinedDeployment:\n",
        "\n",
        "        MODEL_NAME: str = model_name\n",
        "        DEFAULT_VERSION: str = default_version\n",
        "\n",
        "        @serve.multiplexed(max_num_models_per_replica=2)\n",
        "        async def get_model(self, model_version: str):\n",
        "            model = mlflow.pyfunc.load_model(f\"models:/{self.MODEL_NAME}/{model_version}\")\n",
        "\n",
        "            if model.metadata.get_model_info().signature != model_info.signature:\n",
        "                raise ValueError(f\"Requested version {model_version} has signature incompatible with that of default version {self.DEFAULT_VERSION}\")\n",
        "            return model\n",
        "\n",
        "        # TODO: Extend this to support batching (lists of inputs and outputs)\n",
        "        @app.post(\"/serve\", response_model=List[output_datamodel])\n",
        "        async def serve(self, model_input: input_datamodel, response: Response):\n",
        "            model_id = serve.get_multiplexed_model_id()\n",
        "            if model_id == \"\":\n",
        "                model_id = self.DEFAULT_VERSION\n",
        "\n",
        "            try:\n",
        "                model = await self.get_model(model_id)\n",
        "            except ValueError:\n",
        "                response.status_code = status.HTTP_409_CONFLICT\n",
        "                return [{\"translation_text\": \"FAILED\"}]\n",
        "\n",
        "            return model.predict(model_input.dict())\n",
        "\n",
        "    return DynamicallyDefinedDeployment\n",
        "\n",
        "deployment = deployment_from_model_name(\"translation_model\", default_version=en_to_fr_version)\n",
        "\n",
        "serve.run(deployment.bind(), blocking=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x911zDhomWMj",
        "outputId": "7dc78df7-4f06-4871-d45f-37cfb852ffc5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(ServeReplica:default:DynamicallyDefinedDeployment pid=33001)\u001b[0m INFO 2024-12-23 16:03:30,503 default_DynamicallyDefinedDeployment iwidgax2 8989a73b-3173-48d0-a0dc-d301363e731c -- POST /serve/ 307 10.8ms\n",
            "\u001b[36m(ServeReplica:default:DynamicallyDefinedDeployment pid=33001)\u001b[0m INFO 2024-12-23 16:03:30,544 default_DynamicallyDefinedDeployment iwidgax2 e00d9137-a259-4954-8a12-81a3314bc5d2 -- Loading model '15'.\n",
            "\u001b[36m(ServeReplica:default:DynamicallyDefinedDeployment pid=33001)\u001b[0m 2024-12-23 16:03:38.056305: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\u001b[36m(ServeReplica:default:DynamicallyDefinedDeployment pid=33001)\u001b[0m 2024-12-23 16:03:38.085864: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\u001b[36m(ServeReplica:default:DynamicallyDefinedDeployment pid=33001)\u001b[0m 2024-12-23 16:03:38.098177: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\u001b[36m(ServeReplica:default:DynamicallyDefinedDeployment pid=33001)\u001b[0m 2024-12-23 16:03:39.580308: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(ServeReplica:default:DynamicallyDefinedDeployment pid=33001)\u001b[0m Device set to use cpu\n",
            "\u001b[36m(ServeReplica:default:DynamicallyDefinedDeployment pid=33001)\u001b[0m INFO 2024-12-23 16:03:50,242 default_DynamicallyDefinedDeployment iwidgax2 e00d9137-a259-4954-8a12-81a3314bc5d2 -- Successfully loaded model '15' in 19697.5ms.\n",
            "\u001b[36m(ServeReplica:default:DynamicallyDefinedDeployment pid=33001)\u001b[0m <ipython-input-87-4c3da1a45bfd>:40: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'translation_text': \"Le temps est beau aujourd'hui\"}]"
            ]
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "resp = requests.post(\n",
        "    \"http://127.0.0.1:8000/serve/\",\n",
        "    json={\"prompt\": \"The weather is lovely today\"},\n",
        ")\n",
        "\n",
        "assert resp.ok\n",
        "assert resp.status_code == 200\n",
        "\n",
        "resp.json()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EX7ff2wg5PjL",
        "outputId": "edf0587a-abf5-4160-a621-f9ac4faee6bf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(ServeReplica:default:DynamicallyDefinedDeployment pid=33001)\u001b[0m INFO 2024-12-23 16:03:57,563 default_DynamicallyDefinedDeployment iwidgax2 df6b7526-edee-486a-a06e-f15407d4e1aa -- POST /serve/ 307 7.2ms\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'translation_text': \"Le temps est beau aujourd'hui\"}]"
            ]
          },
          "execution_count": 89,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "resp = requests.post(\n",
        "    \"http://127.0.0.1:8000/serve/\",\n",
        "    json={\"prompt\": \"The weather is lovely today\"},\n",
        "    headers={\"serve_multiplexed_model_id\": str(en_to_fr_version)},\n",
        ")\n",
        "\n",
        "assert resp.ok\n",
        "assert resp.status_code == 200\n",
        "\n",
        "resp.json()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwkDDzebG_dd"
      },
      "source": [
        "Let's now confirm that the signature-check provision we put in place actually works. For this, let's register this same model with a **slightly** different signature. This should be enough to trigger the failsafe.\n",
        "\n",
        "(Remember when we made the input label configurable at the start of this exercise? This is where that finally comes into play. 😎)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYydMogXHsOJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "with mlflow.start_run():\n",
        "    incompatible_version = str(mlflow.pyfunc.log_model(\n",
        "        \"translation_model\",\n",
        "        registered_model_name=\"translation_model\",\n",
        "        python_model=MyTranslationModel(),\n",
        "        pip_requirements=[\"transformers\"],\n",
        "        input_example=pd.DataFrame({\n",
        "            \"text_to_translate\": [\n",
        "                \"Hello my name is Jon.\",\n",
        "            ],\n",
        "        }),\n",
        "        model_config={\n",
        "            \"input_label\": \"text_to_translate\",\n",
        "            \"hfhub_name\": \"google-t5/t5-base\",\n",
        "            \"lang_from\": \"en\",\n",
        "            \"lang_to\": \"de\",\n",
        "        },\n",
        "    ).registered_model_version)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Yn-5VlIH6gs"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "resp = requests.post(\n",
        "    \"http://127.0.0.1:8000/serve/\",\n",
        "    json={\"prompt\": \"The weather is lovely today\"},\n",
        "    headers={\"serve_multiplexed_model_id\": incompatible_version},\n",
        ")\n",
        "assert not resp.ok\n",
        "resp.status_code == 409\n",
        "\n",
        "assert resp.json()[0][\"translation_text\"] == \"FAILED\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMhjLZh-jCVa"
      },
      "source": [
        "(The technically \"correct\" thing to do here would be to implement a response container that allows for an \"error message\" to be defined as part of the actual response, rather than \"abusing\" the `translation_text` field like we do here. For demonstration purposes, however, this'll do.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCLtQCgsjwPM"
      },
      "source": [
        "To fully close things out, let's try registering an entirely different model -- with an entirely different signature -- and deploying that via `deployment_from_model_name()`. This will help us confirm that the entire signature is defined from the loaded model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXUPRszjIGYN"
      },
      "outputs": [],
      "source": [
        "import mlflow\n",
        "from transformers import pipeline\n",
        "\n",
        "class QuestionAnswererModel(mlflow.pyfunc.PythonModel):\n",
        "    def load_context(self, context):\n",
        "\n",
        "        self.model_context = context.model_config.get(\n",
        "            \"model_context\",\n",
        "            \"My name is Hans and I live in Germany.\",\n",
        "        )\n",
        "        self.model_name = context.model_config.get(\n",
        "            \"model_name\",\n",
        "            \"deepset/roberta-base-squad2\",\n",
        "        )\n",
        "\n",
        "        self.tokenizer_name = context.model_config.get(\n",
        "            \"tokenizer_name\",\n",
        "            \"deepset/roberta-base-squad2\",\n",
        "        )\n",
        "\n",
        "        self.pipeline = pipeline(\n",
        "            \"question-answering\",\n",
        "            model=self.model_name,\n",
        "            tokenizer=self.tokenizer_name,\n",
        "        )\n",
        "\n",
        "    def predict(self, context, model_input, params=None):\n",
        "        resp = self.pipeline(\n",
        "            question=model_input[\"question\"].tolist(),\n",
        "            context=self.model_context,\n",
        "        )\n",
        "\n",
        "        return [resp] if type(resp) is not list else resp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_p4FrmmhPAuq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "with mlflow.start_run():\n",
        "    model_info = mlflow.pyfunc.log_model(\n",
        "        \"question_answerer\",\n",
        "        registered_model_name=\"question_answerer\",\n",
        "        python_model=QuestionAnswererModel(),\n",
        "        pip_requirements=[\"transformers\"],\n",
        "        input_example=pd.DataFrame({\n",
        "            \"question\": [\n",
        "                \"Where do you live?\",\n",
        "                \"What is your name?\",\n",
        "            ],\n",
        "        }),\n",
        "        model_config={\n",
        "            \"model_context\": \"My name is Hans and I live in Germany.\",\n",
        "        },\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-0mQytrKyOc",
        "outputId": "dd59ef90-ed96-490a-c27f-8f5dbc023ed3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "inputs: \n",
              "  ['question': string (required)]\n",
              "outputs: \n",
              "  ['score': double (required), 'start': long (required), 'end': long (required), 'answer': string (required)]\n",
              "params: \n",
              "  None"
            ]
          },
          "execution_count": 117,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_info.signature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afpSjdgYPaCw"
      },
      "outputs": [],
      "source": [
        "from ray import serve\n",
        "\n",
        "serve.run(\n",
        "    deployment_from_model_name(\n",
        "        \"question_answerer\",\n",
        "        default_version=str(model_info.registered_model_version),\n",
        "    ).bind(),\n",
        "    blocking=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MsLq5vbsS84T",
        "outputId": "73489ce0-984b-4915-e8e0-27db7a8966ec"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(ServeReplica:default:DynamicallyDefinedDeployment pid=35834)\u001b[0m INFO 2024-12-23 16:14:40,551 default_DynamicallyDefinedDeployment z6r4w9bp f766b328-7f11-467b-b6fa-04f6d6c17a84 -- POST /serve/ 307 8.6ms\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'score': 3.255750561947934e-05,\n",
              "  'start': 30,\n",
              "  'end': 38,\n",
              "  'answer': 'Germany.'}]"
            ]
          },
          "execution_count": 130,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(ServeReplica:default:DynamicallyDefinedDeployment pid=35834)\u001b[0m INFO 2024-12-23 16:14:42,857 default_DynamicallyDefinedDeployment z6r4w9bp 74527eca-776e-497f-b478-b4dc8e24f53a -- POST /serve 200 2181.2ms\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "resp = requests.post(\n",
        "    \"http://127.0.0.1:8000/serve/\",\n",
        "    json={\"question\": \"The weather is lovely today\"},\n",
        ")\n",
        "resp.json()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "32dc3acc814944d4838d029f029c0dd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b7dd8cc512a14d44a2dd4ad391326f49",
              "IPY_MODEL_6331ddfcd8884f489046579014469b9b",
              "IPY_MODEL_25e660422d374df7bfec9ffd20a20b50"
            ],
            "layout": "IPY_MODEL_ac8ce7809cd44cf7b0f241930e4f6ac8"
          }
        },
        "b7dd8cc512a14d44a2dd4ad391326f49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_381849c7134e435d840a813aed7d0171",
            "placeholder": "​",
            "style": "IPY_MODEL_d343cfa03753414d8cc58732cc96abaa",
            "value": "config.json: 100%"
          }
        },
        "6331ddfcd8884f489046579014469b9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_afcee86421514eba93585650c886385b",
            "max": 1208,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a206f552c0354964b5bee0dd36a9a075",
            "value": 1208
          }
        },
        "25e660422d374df7bfec9ffd20a20b50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_65dbde4fd56f43ff9434a3e5e425c01b",
            "placeholder": "​",
            "style": "IPY_MODEL_6b7b9e25a612468b9f4bdf3021b5fdf1",
            "value": " 1.21k/1.21k [00:00&lt;00:00, 38.6kB/s]"
          }
        },
        "ac8ce7809cd44cf7b0f241930e4f6ac8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "381849c7134e435d840a813aed7d0171": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d343cfa03753414d8cc58732cc96abaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "afcee86421514eba93585650c886385b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a206f552c0354964b5bee0dd36a9a075": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "65dbde4fd56f43ff9434a3e5e425c01b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b7b9e25a612468b9f4bdf3021b5fdf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c6aa0fd4f8604fd1a8091b07f6b4ebcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a876636a5b4f415b9f3eb261433947f8",
              "IPY_MODEL_bca38994e0064985ab91d38548af0472",
              "IPY_MODEL_59d1e0dca5df409b84655b730b44f5b7"
            ],
            "layout": "IPY_MODEL_76d9fbf5589c456fa215f0432cd4586e"
          }
        },
        "a876636a5b4f415b9f3eb261433947f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab83433647b04558a0d05ba2b9d89319",
            "placeholder": "​",
            "style": "IPY_MODEL_36008e0b1d0741688e8faa3590284c99",
            "value": "model.safetensors: 100%"
          }
        },
        "bca38994e0064985ab91d38548af0472": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17b21fe12b9c44898bfdbbf027e4729a",
            "max": 891646390,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_97bef391062a493c82f7b0ac45b06870",
            "value": 891646390
          }
        },
        "59d1e0dca5df409b84655b730b44f5b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_57602187346c4a7ab9ee8b2d70b657db",
            "placeholder": "​",
            "style": "IPY_MODEL_6fb7db576be647c98d1ac2d5d7f6590f",
            "value": " 892M/892M [00:05&lt;00:00, 157MB/s]"
          }
        },
        "76d9fbf5589c456fa215f0432cd4586e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab83433647b04558a0d05ba2b9d89319": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36008e0b1d0741688e8faa3590284c99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "17b21fe12b9c44898bfdbbf027e4729a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97bef391062a493c82f7b0ac45b06870": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "57602187346c4a7ab9ee8b2d70b657db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6fb7db576be647c98d1ac2d5d7f6590f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1a159611f43b4ae1887499e7706e5e21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c953b9fa725540a58d4b3adca3cde382",
              "IPY_MODEL_55d6d9c832454cfbaf451047253acb8b",
              "IPY_MODEL_0dd58e9e118248a783d97dfe4660dafc"
            ],
            "layout": "IPY_MODEL_7e1e55b87627434085df121c255f4571"
          }
        },
        "c953b9fa725540a58d4b3adca3cde382": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_07327813872f4da49192b311bdf6ca16",
            "placeholder": "​",
            "style": "IPY_MODEL_97451267b0644213a14e1c132f8e32a0",
            "value": "generation_config.json: 100%"
          }
        },
        "55d6d9c832454cfbaf451047253acb8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_abac47606fc4476e8f809df304666969",
            "max": 147,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ded0dbe1e8634eefa56f6a7c2f727644",
            "value": 147
          }
        },
        "0dd58e9e118248a783d97dfe4660dafc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_826946eb7d7d4c4198b5b3dc4f486f5c",
            "placeholder": "​",
            "style": "IPY_MODEL_93dc42a292de44078b8303a5e3f77a5d",
            "value": " 147/147 [00:00&lt;00:00, 8.78kB/s]"
          }
        },
        "7e1e55b87627434085df121c255f4571": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07327813872f4da49192b311bdf6ca16": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97451267b0644213a14e1c132f8e32a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "abac47606fc4476e8f809df304666969": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ded0dbe1e8634eefa56f6a7c2f727644": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "826946eb7d7d4c4198b5b3dc4f486f5c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93dc42a292de44078b8303a5e3f77a5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c5b79f27c8834eeeaa058f2d828f7609": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d8e521ec0d4443cab866d8ab58a72acb",
              "IPY_MODEL_f28587577722483b93df1ad78265daa7",
              "IPY_MODEL_244c671669654875ac39015db4e40a2b"
            ],
            "layout": "IPY_MODEL_de20e832dd7447998e247ec2b11d9137"
          }
        },
        "d8e521ec0d4443cab866d8ab58a72acb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd7ce0f777c846cd9cc01d136524e790",
            "placeholder": "​",
            "style": "IPY_MODEL_6dc1ae8f8c894145b68a1b964add2593",
            "value": "spiece.model: 100%"
          }
        },
        "f28587577722483b93df1ad78265daa7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a7c65a8bed184d6f99257fc8dd05fa1a",
            "max": 791656,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_77cd41f4b1f948bf9a580f89812ee888",
            "value": 791656
          }
        },
        "244c671669654875ac39015db4e40a2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df247f98d36644588b3e0f2d3a4f7216",
            "placeholder": "​",
            "style": "IPY_MODEL_7ee3568f2a5c43d3adfc8b1e7f50425f",
            "value": " 792k/792k [00:00&lt;00:00, 6.14MB/s]"
          }
        },
        "de20e832dd7447998e247ec2b11d9137": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd7ce0f777c846cd9cc01d136524e790": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6dc1ae8f8c894145b68a1b964add2593": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a7c65a8bed184d6f99257fc8dd05fa1a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77cd41f4b1f948bf9a580f89812ee888": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "df247f98d36644588b3e0f2d3a4f7216": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ee3568f2a5c43d3adfc8b1e7f50425f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "89d8044972fd45dda88ecfdd486b253c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_89aa211f82344388917c723c972caed2",
              "IPY_MODEL_19a572661cb548f898bac60c3db071a2",
              "IPY_MODEL_37453595a40f4341bbdd5f73fc99c09f"
            ],
            "layout": "IPY_MODEL_f4ab17cc1e87461a96e3fd3ade429212"
          }
        },
        "89aa211f82344388917c723c972caed2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_274629bce09e46d28e82fdc156ce4c17",
            "placeholder": "​",
            "style": "IPY_MODEL_24551216ab5a405ca47cc1d34224916b",
            "value": "tokenizer.json: 100%"
          }
        },
        "19a572661cb548f898bac60c3db071a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ffd4333705d44b14b4f58c370fc3faa1",
            "max": 1389353,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_94468a5db6414622a285e1d54a41d712",
            "value": 1389353
          }
        },
        "37453595a40f4341bbdd5f73fc99c09f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0061c8b5ea944f6ea504984f1b829f79",
            "placeholder": "​",
            "style": "IPY_MODEL_6891f9df939f4c2c8ac6b7829750ee2b",
            "value": " 1.39M/1.39M [00:00&lt;00:00, 5.46MB/s]"
          }
        },
        "f4ab17cc1e87461a96e3fd3ade429212": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "274629bce09e46d28e82fdc156ce4c17": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24551216ab5a405ca47cc1d34224916b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ffd4333705d44b14b4f58c370fc3faa1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94468a5db6414622a285e1d54a41d712": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0061c8b5ea944f6ea504984f1b829f79": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6891f9df939f4c2c8ac6b7829750ee2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}